# PNS-2019 (National Health Survey 2019), Prediction of Heart Disease by LGBM Modeling

National Health Survey (PNS-2019) is a population-based survey conducted by the Brazilian Institute of Geography and Statistics (IBGE). PNS-2019 is an official program developed by Brazilian Health Minister. It has the main objective to take information about Brazilian residents regarding their health-related risk behaviors, chronic health conditions, use of the public healthcare system, and other information. PNS-2019 subsidizes public policies, either evaluating the current ones or promoting others to improve the quality of health for the entire population.

PNS-2019 has similarities with the Behavioral Risk Factor Surveillance System (BRFSS), developed by CDC (Center for Disease Control and Prevention, USA) (https://www.cdc.gov/brfss/index.html). 

**PNS-2019 Dataset**: raw data is available through the link: (https://www.ibge.gov.br/en/statistics/social/health/16840-national-survey-of-health.html?=&t=resultados)

The later addition of PNS was in 2019, which covered 293,726 interviews covering all Brazilian regions. The questionnaire involved different features that somehow impact the health quality of residents, such as access to basic health assistance, lifestyles, socioeconomic aspect, chronic diseases, violence, and others.
Among all the chronic diseases covered in the current PNS-2019, heart disease was surveyed deeper; It makes sense once heart disease and stroke are the top causes of death worldwide. The same trend is also noted in Brazil.

**Objective**: From data generated by the PNS-2019, the present repository proposes a machine learning (ML) model to predict the probability of someone developing heart disease.

**Pre-processing – PNS-2019**: The PNS-2019 raw data file corresponds to 293,726 rows and 1,088 columns. Carried out by IBGE using their sampling survey methodologies. From Data related to chronic diseases, lifestyles, risk factors, and socioeconomic aspects of the person that answered the questionary were filtered into a database with 94,114 rows and 399 columns (features) called * *df_pns_rawdata* *

The next step was to clean up, dropping all lines with data missing. As the data source comes from a survey (questionary answered by different people), it looks that filling it with any input resource doesn’t make sense. At the end of that step, the database was reduced to 81,218 rows × 55 columns. It is called * *df_pns_model* *. See: the feature dictionary table attached

Features selection were based on the three pillars: 
1. general information about the interviewed; 
2. all chronic diseases covered by the PNS-219;
3. risk factors for heart diseases recommended by CDC Center for Disease Control and Prevention, USA (https://www.cdc.gov/chronicdisease/resources/publications/factsheets/heart-disease-stroke.htm) and WHO World Health Organization (https://www.who.int/health-topics/cardiovascular-diseases#tab=tab_1).

**Prediction of heart disease**
The survey (PNS-2019) questioned people in their houses, meaning the answers came from the interviews instead of the health care system. So, consider the possibility of bias in the responses once there is no medical accreditation.

**Modeling Performance Metrics**
Generally, a classification model is evaluated by the accuracy of both training and test datasets. As noted, the current database is imbalanced, so other metrics might be necessary to obtain a better performance estimation of the model to classify appropriately. 
From the perspective of the imbalanced dataset, the following metrics were used to evaluate the model:
1. Precision —Precision explains how many correctly predicted cases were positive. It is helpful in cases where a False Positive is more concerned than a False Negative.
2. Recall (Sensitivity) — Recall explains how many positive cases the model predicts correctly. It is a valuable metric in cases where a False Negative is of a more profound concern than a False Positive.
3. AUC-ROC (Receiver Operator Characteristic) is a probability curve that plots the True Positive Rate against the False Positive Rate at different threshold values. AUC (Area Under the Curve) is the measurement of the ability of a classifier to distinguish between classes.
In a medical case like that, typically, the concern is to predict a positive case correctly, even overestimating the false positive. So, Recall and AUC-ROC come first, then Precision. In that way, the current modeling proposed was evaluated

**Machine Learning Package Tools**:
**Data Transformation**:
+ Categorical features, as shown in the variable dictionary, were transformed into numerical applying One Hot Encoder Transformer (https://scikit-		learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html); 
+ Numerical features, as also shown in the variable dictionary, were scaled using the Standard Scaler (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) 

**Modeling**:
+ LBGM Classifier for the train modeling (https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html); 
+ Grid-Search for hyperparameters optimization (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html); 
+ Cross-validation, K-fold (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)

**Data Visualization**
Plotty (https://plotly.com/python/), and Seaborn (https://seaborn.pydata.org/)

**Modeling** 
LGBM (https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html), LightGBM, Light Gradient-boosting Machine, is a free and open-source distributed gradient-boosting framework for machine learning, originally developed by Microsoft. It is based on decision tree algorithms and used for ranking, classification, and other machine learning tasks.

Start by * *splitting the dataset* * into train and test at a rate of 70% and 30%, respectively. Applying OneHot Encoder for categorical features and Standard Scaler for numerical features transformed the train and test dataset. * *LGBM Classifier parameters were class_weight={0:1,1:16}, learning_rate=0.02, 
max_depth=9, n_estimators=300* * as the best parameter obtained and used for the training.

**Performance obtained on the train dataset** = * *RoC-AUC:0.846; Recall:0.770; Precision 0.1653 for the test set RoC-AUC:0.782; Recall:0.677; Precision:0.140* *
